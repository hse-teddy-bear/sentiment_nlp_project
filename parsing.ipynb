{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da887a10",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpulse import TinkoffPulse\n",
    "from time import sleep\n",
    "from pytz import timezone\n",
    "from httpx import HTTPStatusError\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd1b20",
   "metadata": {},
   "source": [
    "### **Parsing Data by tickers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse = TinkoffPulse()\n",
    "\n",
    "def parsing_tpulse(ticker, N, KEYS):\n",
    "\n",
    "    cursor = 999999999\n",
    "    raw_data = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        try:\n",
    "            response = pulse.get_posts_by_ticker(ticker, cursor)\n",
    "            try:\n",
    "                cursor = response[\"nextCursor\"]\n",
    "            except:\n",
    "                print('Error: last_cursor')\n",
    "                return raw_data\n",
    "            posts = response[\"items\"]\n",
    "            for post in posts:\n",
    "                data = {\n",
    "                    key: post[key] for key in KEYS\n",
    "                }\n",
    "                data['text'] = post['content']['text']\n",
    "                data['reactions_counters'] = post['reactions']['counters']\n",
    "                raw_data.append(data)\n",
    "\n",
    "        except HTTPStatusError:\n",
    "            pass\n",
    "        sleep(0.25)\n",
    "\n",
    "    print('Parsing finished.')\n",
    "\n",
    "    print('Saving data in csv...')\n",
    "    result_df = pd.DataFrame(raw_data)\n",
    "    result_df['inserted'] = pd.to_datetime(result_df[\"inserted\"].str[:10])\n",
    "    result_df.to_csv(f'df_{ticker}_data.csv', encoding='utf-8-sig', index=False)\n",
    "    print('Saving finished.')\n",
    "\n",
    "    return raw_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda2a7d",
   "metadata": {},
   "source": [
    "Поочередно парсим данные по тикерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing finished.\n"
     ]
    }
   ],
   "source": [
    "KEYS = [\n",
    "        \"inserted\",\n",
    "        \"likesCount\",\n",
    "        \"commentsCount\",\n",
    "        ]\n",
    "\n",
    "result = parsing_tpulse(\"SBER\", 5000, KEYS)\n",
    "\n",
    "base_df = pd.DataFrame(result)\n",
    "base_df['inserted'] = pd.to_datetime(base_df[\"inserted\"].str[:10])\n",
    "base_df.to_csv('df_sber_data.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7aba8",
   "metadata": {},
   "source": [
    "### **Text Preprocessing with RegEx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_trf\n",
    "import spacy_transformers\n",
    "import re\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "def split_into_sentences_by_meaning(text):\n",
    "\n",
    "    # Process the text through the spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract sentences based on spaCy's parsing\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "\n",
    "    # Unicode ranges for emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", \n",
    "        flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_companies(text):\n",
    "\n",
    "    # Regex pattern to match words inside curly brackets\n",
    "    pattern = r'\\{([^}]+)\\}'\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_chunk(text, words):\n",
    "    \n",
    "    # Saving list\n",
    "    related_sentence_list = []\n",
    "\n",
    "    # Ensure words list is lowercased for case-insensitive matching\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = split_into_sentences_by_meaning(text)\n",
    "    \n",
    "    # Search for sentences containing any of the words\n",
    "    for sentence in sentences:\n",
    "        if any(word in sentence.lower() for word in words_lower):\n",
    "            related_sentence_list.append(sentence)  # Return the first sentence found that matches\n",
    "    \n",
    "    # If no sentence is found containing any of the words, return an empty string\n",
    "    return '. '.join(related_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pulse_comments(row):\n",
    "\n",
    "    raw_text = row['text']\n",
    "    current_text = remove_emojis(raw_text)\n",
    "    current_text = current_text.lower()\n",
    "\n",
    "    companies_mentioned = find_companies(current_text)\n",
    "    if len(companies_mentioned) <= 1:\n",
    "        return current_text\n",
    "    else: \n",
    "        words = ticker_keywords[ticker]\n",
    "        current_text = current_text.replace('\\n', '. ')\n",
    "        related_text = find_related_chunk(current_text, words)\n",
    "        return related_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_keywords = {\n",
    "    \"tcs\": ['tinkoff', 'tcs', 'tcsg', 'тинькофф', 'ткс', 'тинек', 'тинёк', 'тиньков', 'тинька'],\n",
    "    \"sber\": ['sber', 'sbrf', 'sberbank', 'сбер', 'сбербанк', 'cбера', 'сберу', 'sberp', 'сбербанкa', 'сбербанку', 'сбербанком'],\n",
    "    \"gazp\": ['gazp', 'газпром', 'газпрома', 'газпрому', 'gazprom', 'газпромом', 'gzpr', 'газп', 'gaz'],\n",
    "    \"bane\": ['bashneft', 'башнефть', 'bane', 'баш', 'башнефти', 'башнефтью'],\n",
    "    \"kmaz\": ['kamaz', 'камаз', 'kmaz', 'кмаз', 'камазом', 'камаза', 'камазу'],\n",
    "    \"mvid\": ['mvideo', 'мвидео', 'м-видео', 'mvid', 'мвид', 'эмвидео', 'м видео'],\n",
    "    \"pikk\": ['pik', 'пик', 'pikk', 'пикк'],\n",
    "    \"rtkm\": ['rostelecom', 'ростелеком', 'rtkm', 'рткм', 'ростел', 'ростелекома', 'ростелекому', 'ростелекомом'],\n",
    "    \"sgzh\": ['sgzh', 'сгж', 'сегежа', 'segezha', 'сегежи', 'сегежей', 'сегежу'],\n",
    "    \"yndx\": ['yandex', 'яндекс', 'yndx', 'индекс', 'yndex', 'яшка', 'яндекса', 'яндексу', 'яндексом']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: sber\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in os.listdir('raw_data'):\n",
    "        \n",
    "    if dataset_name in ['df_SBER_data.csv']:\n",
    "\n",
    "        df = pd.read_csv(rf'raw_data/{dataset_name}')\n",
    "        \n",
    "        ticker = dataset_name.split(\"_\")[1].lower()\n",
    "\n",
    "        df['text_preprocessed'] = df.apply(preprocess_pulse_comments, axis=1)\n",
    "\n",
    "        df.to_csv(f'preprocessed_data_full/df_{ticker}_full.csv', index=False)\n",
    "\n",
    "        print(f'Done: {ticker}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
